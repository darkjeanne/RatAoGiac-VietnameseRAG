"""
H·ªá th·ªëng advanced-RAG v·ªõi c√°c c·∫£i ti·∫øn nh∆∞ sau:
- Parent Document Retrieval (Truy xu·∫•t t√†i li·ªáu g·ªëc/cha)
- C·∫£i ti·∫øn truy xu·∫•t: Hybrid Search v√† Re-ranking
- T·ªëi ∆∞u h√≥a t·∫°o sinh: Contextual COmpression v√† Advanced Promt
"""
import streamlit as st
import os
import pickle 
import re
import uuid 

# Import c√°c l·ªõp LangChain c·∫ßn thi·∫øt
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank
from langchain.storage import InMemoryStore
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Th∆∞ vi·ªán ƒë·ªçc PDF
import PyPDF2

COHERE_API_KEY = "LSsVgNhEJmAWepOBmRNcSwXABh18VtmMXWYGLpD2" 

# --- C·∫•u h√¨nh c·ªë ƒë·ªãnh ---
embedding_model_name = "bkai-foundation-models/vietnamese-bi-encoder"
ollama_model_name = "llama3:8b-instruct-q4_0"

# --- H√†m x·ª≠ l√Ω t√†i li·ªáu ---
def process_uploaded_file(uploaded_file):
    """ƒê·ªçc file v√† tr·∫£ v·ªÅ m·ªôt list ch·ª©a Document object th√¥."""
    text = ""
    if uploaded_file is not None:
        try:
            file_extension = os.path.splitext(uploaded_file.name)[1].lower()
            if file_extension == ".txt":
                text = str(uploaded_file.read(), "utf-8")
            elif file_extension == ".pdf":
                pdf_reader = PyPDF2.PdfReader(uploaded_file)
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text()
            else:
                st.error("ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng t·∫£i file .txt ho·∫∑c .pdf.")
                return None
        except Exception as e:
            st.error(f"L·ªói khi ƒë·ªçc file: {e}")
            return None
        documents = [Document(page_content=text, metadata={"source": uploaded_file.name})]
        return documents
    return None

# --- H√†m kh·ªüi t·∫°o LLM ---
@st.cache_resource(show_spinner="ƒêang kh·ªüi t·∫°o m√¥ h√¨nh AI...")
def get_llm_and_prompt(_ollama_model_name):
    """Kh·ªüi t·∫°o LLM v√† Prompt Template T·ªêI ∆ØU H√ìA."""
    try:
        llm_instance = ChatOllama(model=_ollama_model_name, temperature=0.2)
        # --- PROMPT N√ÇNG C·∫§P ---
        prompt_template_str = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp v√† th√¥ng th√°i. Vai tr√≤ c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c v√† s√∫c t√≠ch, ch·ªâ d·ª±a v√†o n·ªôi dung trong "Th√¥ng tin tham kh·∫£o" ƒë∆∞·ª£c cung c·∫•p.

**Quy tr√¨nh l√†m vi·ªác c·ªßa b·∫°n nh∆∞ sau:**
1.  **Suy nghƒ© n·ªôi b·ªô (kh√¥ng hi·ªÉn th·ªã ph·∫ßn n√†y):** ƒê·ªçc k·ªπ c√¢u h·ªèi v√† to√†n b·ªô "Th√¥ng tin tham kh·∫£o". Trong ƒë·∫ßu, h√£y x√°c ƒë·ªãnh v√† g·∫°ch ch√¢n nh·ªØng c√¢u, nh·ªØng d·ªØ ki·ªán t·ª´ t√†i li·ªáu c√≥ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn c√¢u h·ªèi.
2.  **T·ªïng h·ª£p v√† tr·∫£ l·ªùi (ch·ªâ hi·ªÉn th·ªã ph·∫ßn n√†y):** D·ª±a tr√™n nh·ªØng th√¥ng tin li√™n quan ƒë√£ x√°c ƒë·ªãnh ·ªü b∆∞·ªõc 1, h√£y so·∫°n m·ªôt c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh, m·∫°ch l·∫°c v√† t·ª± nhi√™n cho ng∆∞·ªùi d√πng.
    - ƒêi th·∫≥ng v√†o v·∫•n ƒë·ªÅ, kh√¥ng d√πng c√°c c·ª•m t·ª´ m·ªü ƒë·∫ßu nh∆∞ "D·ª±a tr√™n t√†i li·ªáu..." hay "Theo th√¥ng tin ƒë∆∞·ª£c cung c·∫•p...".
    - Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi nh∆∞ m·ªôt chuy√™n gia ƒëang gi·∫£i th√≠ch v·∫•n ƒë·ªÅ.
    - N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu, ch·ªâ c·∫ßn tr·∫£ l·ªùi r·∫±ng: "Th√¥ng tin n√†y kh√¥ng c√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p."
    - Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.

Th√¥ng tin tham kh·∫£o:
{context}<|eot_id|><|start_header_id|>user<|end_header_id|>
C√¢u h·ªèi: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
C√¢u tr·∫£ l·ªùi h·ªØu √≠ch:"""

        PROMPT = PromptTemplate(template=prompt_template_str, input_variables=["context", "question"])
        chain_type_kwargs = {"prompt": PROMPT}
        return llm_instance, chain_type_kwargs
    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o LLM: {e}")
        return None, None

# --- Giao di·ªán Streamlit ---
st.set_page_config(page_title="Chatbot T√†i Li·ªáu RAG", layout="wide")
st.title("üí¨ Chatbot H·ªèi ƒê√°p T√†i Li·ªáu N√¢ng Cao")
st.markdown("T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n ( .txt ho·∫∑c .pdf) v√† ƒë·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung ƒë√≥.")

# --- Thanh b√™n (Sidebar) ---
with st.sidebar:
    st.header("üìÅ T·∫£i T√†i Li·ªáu")
    uploaded_file = st.file_uploader("Ch·ªçn m·ªôt file .txt ho·∫∑c .pdf", type=["txt", "pdf"])
    if uploaded_file:
        st.success(f"ƒê√£ t·∫£i l√™n file: {uploaded_file.name}")
    else:
        st.info("Vui l√≤ng t·∫£i l√™n m·ªôt t√†i li·ªáu ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

# --- X·ª≠ l√Ω v√† kh·ªüi t·∫°o ---
# Kh·ªëi n√†y ƒë∆∞·ª£c t√°i c·∫•u tr√∫c ho√†n to√†n ƒë·ªÉ t√≠ch h·ª£p t·∫•t c·∫£ c√°c k·ªπ thu·∫≠t
@st.cache_resource(show_spinner="ƒêang x·ª≠ l√Ω t√†i li·ªáu v√† x√¢y d·ª±ng pipeline RAG...")
def build_full_rag_pipeline(_raw_documents):
    # --- 1: LOGIC C·ª¶A PARENT DOCUMENT ---
    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)

    parent_chunks = parent_splitter.split_documents(_raw_documents)
    
    docstore = InMemoryStore()
    parent_chunk_ids = [str(uuid.uuid4()) for _ in parent_chunks]
    docstore.mset(list(zip(parent_chunk_ids, parent_chunks)))

    child_chunks = []
    for i, p_chunk in enumerate(parent_chunks):
        _child_chunks = child_splitter.split_documents([p_chunk])
        for c_chunk in _child_chunks:
            c_chunk.metadata["parent_id"] = parent_chunk_ids[i]
        child_chunks.extend(_child_chunks)

    # --- 2: LOGIC C·ª¶A HYBRID SEARCH (tr√™n chunk con) ---
    embeddings_model_instance = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cuda'})
    vectorstore = FAISS.from_documents(child_chunks, embeddings_model_instance)
    
    faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    bm25_retriever = BM25Retriever.from_documents(child_chunks)
    bm25_retriever.k = 10
    
    hybrid_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])

    # --- 3: K·∫æT H·ª¢P PARENT RETRIEVAL V√Ä HYBRID SEARCH ---
    def get_parent_chunks(child_docs):
        parent_ids = {doc.metadata["parent_id"] for doc in child_docs}
        return [doc for doc in docstore.mget(list(parent_ids)) if doc is not None]

    parent_retriever_chain = hybrid_retriever | RunnableLambda(get_parent_chunks)

    # --- 4: LOGIC C·ª¶A RE-RANKING (tr√™n chunk cha) ---
    reranker = CohereRerank(cohere_api_key=COHERE_API_KEY, model="rerank-multilingual-v3.0", top_n=3)
    final_retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=parent_retriever_chain)
    
    return final_retriever

retriever = None
llm, chain_type_kwargs = None, None

if uploaded_file:
    raw_documents = process_uploaded_file(uploaded_file)
    if raw_documents:
        retriever = build_full_rag_pipeline(raw_documents)
        llm, chain_type_kwargs = get_llm_and_prompt(ollama_model_name)

# --- Kh·ªüi t·∫°o v√† hi·ªÉn th·ªã l·ªãch s·ª≠ chat ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! H√£y t·∫£i t√†i li·ªáu l√™n v√† ƒë·∫∑t c√¢u h·ªèi cho t√¥i nh√©."}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and "sources" in message:
            with st.expander("Xem ngu·ªìn tham kh·∫£o"):
                for i, source in enumerate(message["sources"]):
                    # Metadata c·ªßa chunk cha kh√¥ng c√≥ "chunk_id" nh∆∞ng c√≥ "source"
                    st.caption(f"Ngu·ªìn {i+1} (T·ª´: {source.metadata.get('source', 'N/A')})")
                    st.markdown(source.page_content.replace("\n", " "))

# --- Nh·∫≠n input t·ª´ ng∆∞·ªùi d√πng ---
if prompt := st.chat_input("C√¢u h·ªèi c·ªßa b·∫°n v·ªÅ t√†i li·ªáu..."):
    if not uploaded_file or not retriever or not llm:
        st.warning("Vui l√≤ng t·∫£i l√™n v√† x·ª≠ l√Ω t√†i li·ªáu tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # --- ƒê·ªäNH NGHƒ®A H√ÄM S·∫ÆP X·∫æP L·∫†I CONTEXT ---
        def reorder_documents(docs):
            if not docs:
                return ""
            reordered_docs = []
            if len(docs) > 1:
                reordered_docs.append(docs[0])
                reordered_docs.extend(docs[2:])
                reordered_docs.append(docs[1]) 
            else:
                reordered_docs = docs
            return "\n\n---\n\n".join([doc.page_content for doc in reordered_docs])

        # --- X√ÇY D·ª∞NG QA CHAIN B·∫∞NG LCEL ---
        _, chain_type_kwargs = get_llm_and_prompt(ollama_model_name)
        rag_prompt = chain_type_kwargs["prompt"]
        
        rag_chain = (
            {
                "context": retriever | RunnableLambda(reorder_documents),
                "question": RunnablePassthrough()
            }
            | rag_prompt
            | llm
            | StrOutputParser()
        )
        
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("Bot ƒëang t√¨m ki·∫øm, s·∫Øp x·∫øp v√† suy lu·∫≠n...")
            try:
                source_documents = retriever.invoke(prompt)
                answer = rag_chain.invoke(prompt)
                message_placeholder.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer, "sources": source_documents})

                if source_documents:
                    with st.expander("Xem ngu·ªìn tham kh·∫£o cho c√¢u tr·∫£ l·ªùi n√†y"):
                        for i, source in enumerate(source_documents):
                            st.caption(f"Ngu·ªìn {i+1} (T·ª´: {source.metadata.get('source', 'N/A')})")
                            st.markdown(source.page_content.replace("\n", " "))

            except Exception as e:
                error_message = f"ƒê√£ x·∫£y ra l·ªói: {e}"
                message_placeholder.error(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})