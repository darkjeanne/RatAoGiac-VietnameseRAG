{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-30T10:29:42.097729Z",
     "start_time": "2025-05-30T10:29:41.549307Z"
    }
   },
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import re # Để dùng biểu thức chính quy\n",
    "import pickle # Để lưu Document objects\n",
    "\n",
    "# Hàm để đọc nội dung từ file\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# Đường dẫn đến file văn bản của bạn\n",
    "input_file_path = 'DSTS.txt'\n",
    "novel_text = read_text_file(input_file_path)\n",
    "\n",
    "# --- Cấu hình RecursiveCharacterTextSplitter ---\n",
    "# Các giá trị này có thể cần điều chỉnh sau khi xem kết quả\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # Kích thước mong muốn của mỗi chunk (số ký tự)\n",
    "    chunk_overlap=150,    # Số ký tự chồng lấn giữa các chunk\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"], # Ưu tiên chia theo đoạn, rồi dòng, rồi câu\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "# --- Thực hiện chia chunk và tạo Document objects ---\n",
    "raw_chunks = text_splitter.split_text(novel_text)\n",
    "documents = []\n",
    "current_chapter = None # Biến để lưu trữ chương hiện tại\n",
    "\n",
    "print(f\"Đang xử lý và tạo Document objects cho {len(raw_chunks)} chunks thô...\")\n",
    "\n",
    "for i, chunk_content in enumerate(raw_chunks):\n",
    "    # Cố gắng xác định chương cho chunk này dựa vào sự xuất hiện của \"Chương X\"\n",
    "    # Điều này giả định rằng tiêu đề chương là thứ duy nhất có dạng \"Chương [số]\"\n",
    "    # và nó xuất hiện ở đầu một đoạn văn/chunk.\n",
    "    chapter_match_title = re.match(r\"Chương\\s+(\\d+):?\\s*([^\\n]+)\", chunk_content) # Tìm tiêu đề đầy đủ\n",
    "    chapter_match_simple = re.match(r\"Chương\\s+(\\d+)\", chunk_content) # Tìm \"Chương X\" đơn giản\n",
    "\n",
    "    if chapter_match_title:\n",
    "        current_chapter_number = int(chapter_match_title.group(1))\n",
    "        current_chapter_title = chapter_match_title.group(2).strip()\n",
    "        current_chapter = f\"Chương {current_chapter_number}: {current_chapter_title}\"\n",
    "        # Loại bỏ dòng tiêu đề chương khỏi nội dung chunk nếu nó là dòng duy nhất hoặc rất ngắn\n",
    "        # Hoặc nếu bạn muốn, có thể giữ lại một phần như một dấu hiệu bắt đầu chương.\n",
    "        # Ở đây, chúng ta giả định là muốn loại bỏ hoàn toàn khỏi page_content nếu nó đứng riêng.\n",
    "        # Nếu chunk_content chỉ chứa tiêu đề chương (ví dụ, sau khi split(\"\\n\\n\"))\n",
    "        lines_in_chunk = chunk_content.splitlines()\n",
    "        if len(lines_in_chunk) > 0 and lines_in_chunk[0].strip() == f\"Chương {current_chapter_number}: {current_chapter_title}\":\n",
    "            chunk_content = \"\\n\".join(lines_in_chunk[1:]).strip() # Bỏ dòng tiêu đề\n",
    "        elif len(lines_in_chunk) > 0 and lines_in_chunk[0].strip() == f\"Chương {current_chapter_number}\":\n",
    "             chunk_content = \"\\n\".join(lines_in_chunk[1:]).strip()\n",
    "\n",
    "\n",
    "    elif chapter_match_simple and not chapter_match_title : # Nếu chỉ có \"Chương X\"\n",
    "        current_chapter_number = int(chapter_match_simple.group(1))\n",
    "        current_chapter = f\"Chương {current_chapter_number}\" # Không có tiêu đề cụ thể\n",
    "        lines_in_chunk = chunk_content.splitlines()\n",
    "        if len(lines_in_chunk) > 0 and lines_in_chunk[0].strip() == f\"Chương {current_chapter_number}\":\n",
    "            chunk_content = \"\\n\".join(lines_in_chunk[1:]).strip()\n",
    "\n",
    "    # Nếu chunk quá ngắn sau khi bỏ tiêu đề, có thể bỏ qua hoặc gộp với chunk sau (logic phức tạp hơn)\n",
    "    if not chunk_content.strip(): # Bỏ qua chunk rỗng\n",
    "        continue\n",
    "\n",
    "    doc_metadata = {\n",
    "        \"source\": input_file_path, # Tên file nguồn\n",
    "        \"chunk_id\": i,\n",
    "    }\n",
    "    if current_chapter:\n",
    "        doc_metadata[\"chapter_info\"] = current_chapter\n",
    "\n",
    "\n",
    "    doc = Document(page_content=chunk_content, metadata=doc_metadata)\n",
    "    documents.append(doc)\n",
    "\n",
    "# --- In ra một vài Document đầu tiên để kiểm tra ---\n",
    "print(f\"\\nTổng số Document được tạo ra: {len(documents)}\")\n",
    "print(\"\\n--- 5 Document đầu tiên (sau khi xử lý tiêu đề chương): ---\")\n",
    "for i, doc in enumerate(documents[:5]):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"Content (first 150 chars): {doc.page_content[:150]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "# --- Lưu các Document objects đã xử lý ra file pickle ---\n",
    "output_document_file = 'DSTS_documents_ch1-9.pkl'\n",
    "with open(output_document_file, 'wb') as f:\n",
    "    pickle.dump(documents, f)\n",
    "print(f\"\\nCác Document objects đã được lưu vào file: {output_document_file}\")\n",
    "print(f\"Hãy kiểm tra nội dung của các chunk đầu tiên và metadata để đảm bảo logic xác định chương hoạt động như mong đợi.\")\n",
    "print(\"Bạn có thể cần tinh chỉnh lại `chunk_size`, `chunk_overlap` hoặc logic xử lý tiêu đề chương.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xử lý và tạo Document objects cho 96 chunks thô...\n",
      "\n",
      "Tổng số Document được tạo ra: 96\n",
      "\n",
      "--- 5 Document đầu tiên (sau khi xử lý tiêu đề chương): ---\n",
      "\n",
      "--- Document 1 ---\n",
      "Content (first 150 chars): (Thèm ăn xiên nướng ở mấy hàng bán rong quá đi mất.)\n",
      "Ngẩng đầu nhìn lên bầu trời âm u đầy mây mù, Miêu Miêu trút một tiếng thở dài.\n",
      "Xung quanh cô là m...\n",
      "Metadata: {'source': 'DSTS.txt', 'chunk_id': 0, 'chapter_info': 'Chương 1: Miêu Miêu'}\n",
      "\n",
      "--- Document 2 ---\n",
      "Content (first 150 chars): Còn đối với một dược sư có cuộc sống tương đối ổn như Miêu Miêu thì đây đúng là tay bay vạ gió.\n",
      "Đối với Miêu Miêu, cô chẳng quan tâm lũ người này bắt ...\n",
      "Metadata: {'source': 'DSTS.txt', 'chunk_id': 1, 'chapter_info': 'Chương 1: Miêu Miêu'}\n",
      "\n",
      "--- Document 3 ---\n",
      "Content (first 150 chars): Dù có là chốn lầu son gác tía nơi những bậc cao quý cư ngụ, hay là nơi ngõ liễu tường hoa xô bồ ngoài phố thị, đều không khác gì nhau.\n",
      "Miêu Miêu ôm gi...\n",
      "Metadata: {'source': 'DSTS.txt', 'chunk_id': 2, 'chapter_info': 'Chương 1: Miêu Miêu'}\n",
      "\n",
      "--- Document 4 ---\n",
      "Content (first 150 chars): Nhìn những tấm thẻ gỗ treo trên quai giỏ sẽ thấy hình vẽ cây cỏ cùng vài con số.\n",
      "Trong số các cung nữ cũng có người không biết chữ. Cũng chẳng lạ, bởi...\n",
      "Metadata: {'source': 'DSTS.txt', 'chunk_id': 3, 'chapter_info': 'Chương 1: Miêu Miêu'}\n",
      "\n",
      "--- Document 5 ---\n",
      "Content (first 150 chars): Miêu Miêu là nô tì cấp bậc thấp nhất ở đây, thậm chí còn chưa được nhận chức danh “nữ quan”. Với một cô gái chẳng có ai chống lưng, bị bắt vào đây cho...\n",
      "Metadata: {'source': 'DSTS.txt', 'chunk_id': 4, 'chapter_info': 'Chương 1: Miêu Miêu'}\n",
      "\n",
      "Các Document objects đã được lưu vào file: DSTS_documents_ch1-9.pkl\n",
      "Hãy kiểm tra nội dung của các chunk đầu tiên và metadata để đảm bảo logic xác định chương hoạt động như mong đợi.\n",
      "Bạn có thể cần tinh chỉnh lại `chunk_size`, `chunk_overlap` hoặc logic xử lý tiêu đề chương.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T10:30:06.974403Z",
     "start_time": "2025-05-30T10:29:42.100302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # Sử dụng bản cập nhật\n",
    "from langchain_community.vectorstores import FAISS # Sử dụng bản cập nhật\n",
    "import os # Để kiểm tra file tồn tại\n",
    "\n",
    "# --- Đường dẫn file và tên model ---\n",
    "document_file_path = 'DSTS_documents_ch1-9.pkl'\n",
    "# Chọn một model embedding phù hợp với tiếng Việt và tài nguyên của bạn\n",
    "# \"bkai-foundation-models/vietnamese-bi-encoder\" là một lựa chọn tốt cho tiếng Việt\n",
    "# \"all-MiniLM-L6-v2\" nhanh và nhẹ, nhưng có thể không tối ưu bằng cho tiếng Việt\n",
    "embedding_model_name = \"bkai-foundation-models/vietnamese-bi-encoder\"\n",
    "# embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Đường dẫn để lưu FAISS index\n",
    "faiss_index_output_path = \"DSTS_faiss_index_ch1-9\"\n",
    "\n",
    "# --- 1. Tải lại các Document objects đã được chunk ---\n",
    "if not os.path.exists(document_file_path):\n",
    "    print(f\"Lỗi: File '{document_file_path}' không tồn tại. Vui lòng chạy lại bước chia chunk.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Đang tải Document objects từ '{document_file_path}'...\")\n",
    "with open(document_file_path, 'rb') as f:\n",
    "    documents = pickle.load(f)\n",
    "print(f\"Đã tải {len(documents)} Document objects.\")\n",
    "\n",
    "if not documents:\n",
    "    print(\"Lỗi: Danh sách documents rỗng. Vui lòng kiểm tra lại file pickle.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Khởi tạo mô hình Embedding ---\n",
    "print(f\"Đang khởi tạo mô hình embedding: '{embedding_model_name}'...\")\n",
    "# Sử dụng 'cpu' nếu không có GPU hoặc GPU không đủ mạnh. Thay 'cuda' nếu có.\n",
    "# Lần đầu tải model có thể mất chút thời gian.\n",
    "try:\n",
    "    embeddings_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        model_kwargs={'device': 'cpu'} # Thay 'cuda' nếu bạn có GPU và muốn sử dụng\n",
    "    )\n",
    "    print(\"Mô hình embedding đã được khởi tạo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi khởi tạo mô hình embedding: {e}\")\n",
    "    print(\"Hãy đảm bảo bạn đã cài đặt sentence-transformers và model name là chính xác.\")\n",
    "    print(\"Thử chạy: pip install sentence-transformers\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Tạo Vector Store (FAISS) từ các documents và embeddings model ---\n",
    "# FAISS sẽ tự động tạo embeddings cho các documents.\n",
    "# Quá trình này có thể mất thời gian tùy thuộc vào số lượng chunk và sức mạnh máy tính.\n",
    "print(f\"\\nĐang tạo FAISS index từ {len(documents)} documents...\")\n",
    "try:\n",
    "    vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "    print(\"FAISS index đã được tạo thành công.\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi trong quá trình tạo FAISS index: {e}\")\n",
    "    # In ra một vài document đầu tiên để kiểm tra nội dung\n",
    "    # for i, doc in enumerate(documents[:3]):\n",
    "    #     print(f\"Document {i} content type: {type(doc.page_content)}, metadata type: {type(doc.metadata)}\")\n",
    "    #     print(f\"Document {i} page_content: {doc.page_content[:100]}\")\n",
    "    #     print(f\"Document {i} metadata: {doc.metadata}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 4. Lưu Vector Store ra file để sử dụng sau này ---\n",
    "try:\n",
    "    vector_store.save_local(faiss_index_output_path)\n",
    "    print(f\"FAISS index đã được lưu tại thư mục: '{faiss_index_output_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi lưu FAISS index: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- (Tùy chọn) Kiểm tra thử việc tìm kiếm ---\n",
    "print(\"\\n--- Thử nghiệm tìm kiếm tương đồng ---\")\n",
    "try:\n",
    "    # Đảm bảo vector_store đã được tạo\n",
    "    if 'vector_store' in locals() and vector_store:\n",
    "        test_query = \"Miêu Miêu làm gì ở hậu cung khi mới vào?\"\n",
    "        print(f\"Tìm kiếm cho câu hỏi: '{test_query}'\")\n",
    "        # k=3 nghĩa là lấy 3 kết quả tương đồng nhất\n",
    "        search_results = vector_store.similarity_search_with_score(test_query, k=3)\n",
    "\n",
    "        if search_results:\n",
    "            for i, (doc, score) in enumerate(search_results):\n",
    "                print(f\"\\n--- Kết quả {i+1} (Score: {score:.4f}): ---\")\n",
    "                print(f\"Trích đoạn: {doc.page_content[:250]}...\") # In 250 ký tự đầu\n",
    "                print(f\"Metadata: {doc.metadata}\")\n",
    "        else:\n",
    "            print(\"Không tìm thấy kết quả nào.\")\n",
    "    else:\n",
    "        print(\"Vector store chưa được tạo để thực hiện tìm kiếm.\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi trong quá trình tìm kiếm thử nghiệm: {e}\")\n",
    "\n",
    "print(\"\\nHoàn thành bước tạo và lưu Vector Store!\")"
   ],
   "id": "5646b7dff124185d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải Document objects từ 'DSTS_documents_ch1-9.pkl'...\n",
      "Đã tải 96 Document objects.\n",
      "Đang khởi tạo mô hình embedding: 'bkai-foundation-models/vietnamese-bi-encoder'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_30216\\3095876668.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mô hình embedding đã được khởi tạo.\n",
      "\n",
      "Đang tạo FAISS index từ 96 documents...\n",
      "FAISS index đã được tạo thành công.\n",
      "FAISS index đã được lưu tại thư mục: 'DSTS_faiss_index_ch1-9'\n",
      "\n",
      "--- Thử nghiệm tìm kiếm tương đồng ---\n",
      "Tìm kiếm cho câu hỏi: 'Miêu Miêu làm gì ở hậu cung khi mới vào?'\n",
      "\n",
      "--- Kết quả 1 (Score: 19.6825): ---\n",
      "Trích đoạn: Khi Miêu Miêu nhắc đến chuyện mình đã được nghe Tiểu Lan kể, nữ quan đó liền vui vẻ nhận lời đối với cô.\n",
      "Nơi hậu cung vốn không có chỗ cho tình yêu nam nữ cháy bỏng, nên xem ra ngay cả những kẻ không còn là đàn ông như hoạn quan cũng trở thành đối tư...\n",
      "Metadata: {'source': 'DSTS.txt', 'chunk_id': 15, 'chapter_info': 'Chương 2: Hai vị phi tần'}\n",
      "\n",
      "--- Kết quả 2 (Score: 21.1794): ---\n",
      "Trích đoạn: Khi thời gian để tang kết thúc, các dải băng đen cũng dần biến mất khỏi tầm mắt, tin đồn về Ngọc Diệp phi lại nổi lên. Có vẻ sau khi mất đi Hoàng tử, Hoàng đế quá đau lòng nên đã dồn hết yêu thương cho tiểu công chúa còn sống sót.\n",
      "Nhưng chẳng nghe th...\n",
      "Metadata: {'source': 'DSTS.txt', 'chunk_id': 32, 'chapter_info': 'Chương 4: Nụ cười của tiên nữ'}\n",
      "\n",
      "--- Kết quả 3 (Score: 21.2654): ---\n",
      "Trích đoạn: (Đau đầu, đau bụng, buồn nôn à?)\n",
      "Những triệu chứng này khiến Miêu Miêu nghĩ đến một loại bệnh, nhưng chưa thể chắc chắn được.\n",
      "Cha cô suốt ngày bảo rằng không thể chỉ dựa vào phỏng đoán mà suy xét mọi việc.\n",
      "(Thử đến đó ngó qua một chút xem sao.)\n",
      "Miêu ...\n",
      "Metadata: {'source': 'DSTS.txt', 'chunk_id': 13, 'chapter_info': 'Chương 2: Hai vị phi tần'}\n",
      "\n",
      "Hoàn thành bước tạo và lưu Vector Store!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T14:44:39.666285Z",
     "start_time": "2025-05-30T14:42:59.637735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Không cần thiết cho Ollama nhưng giữ lại nếu bạn có các key khác\n",
    "\n",
    "# Import các lớp cần thiết từ LangChain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_models import ChatOllama # Sử dụng ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate # Để tùy chỉnh prompt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --- Đường dẫn và tên model embedding (GIỮ NGUYÊN) ---\n",
    "faiss_index_path = \"DSTS_faiss_index_ch1-9\"\n",
    "embedding_model_name = \"bkai-foundation-models/vietnamese-bi-encoder\"\n",
    "\n",
    "# --- 1. Tải lại FAISS index và khởi tạo embeddings model (GIỮ NGUYÊN) ---\n",
    "print(f\"Đang khởi tạo mô hình embedding: '{embedding_model_name}'...\")\n",
    "try:\n",
    "    embeddings_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        model_kwargs={'device': 'cpu'} # Hoặc 'cuda'\n",
    "    )\n",
    "    print(\"Mô hình embedding đã được khởi tạo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi khởi tạo mô hình embedding: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nĐang tải FAISS index từ thư mục: '{faiss_index_path}'...\")\n",
    "if not os.path.exists(faiss_index_path):\n",
    "    print(f\"Lỗi: Thư mục FAISS index '{faiss_index_path}' không tồn tại.\")\n",
    "    exit()\n",
    "try:\n",
    "    vector_store = FAISS.load_local(faiss_index_path, embeddings_model, allow_dangerous_deserialization=True)\n",
    "    print(\"FAISS index đã được tải thành công.\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi tải FAISS index: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Khởi tạo LLM (Sử dụng Ollama với Llama 3) ---\n",
    "print(\"\\nĐang khởi tạo LLM (Ollama - Llama 3)...\")\n",
    "# Đảm bảo Ollama server đang chạy và bạn đã pull model này\n",
    "ollama_model_name = \"llama3:8b-instruct-q4_0\" # Hoặc model Llama 3 khác bạn đã pull\n",
    "\n",
    "try:\n",
    "    llm = ChatOllama(\n",
    "        model=ollama_model_name,\n",
    "        temperature=0.2,  # Giảm nhiệt độ để câu trả lời chính xác và bám context hơn\n",
    "        # request_timeout=120 # Tăng timeout nếu model lớn và phản hồi chậm\n",
    "    )\n",
    "    print(f\"LLM Ollama (model: {ollama_model_name}) đã được khởi tạo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi khởi tạo Ollama LLM: {e}\")\n",
    "    print(f\"Hãy đảm bảo Ollama đang chạy trên máy của bạn và model '{ollama_model_name}' đã được tải (ví dụ: ollama pull {ollama_model_name}).\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. (Tùy chọn nhưng KHUYẾN NGHỊ) Tùy chỉnh Prompt cho Llama ---\n",
    "# Llama Instruct models thường hoạt động tốt với một định dạng prompt cụ thể.\n",
    "# Bạn có thể thử nghiệm với prompt mặc định của RetrievalQA trước,\n",
    "# nhưng nếu kết quả chưa tốt, hãy thử tùy chỉnh.\n",
    "\n",
    "# Ví dụ một prompt template có thể phù hợp hơn với Llama Instruct:\n",
    "# (Tham khảo thêm tài liệu của Llama 3 để có prompt tối ưu nhất)\n",
    "prompt_template_str = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Bạn là một trợ lý AI hữu ích. Hãy sử dụng các đoạn thông tin sau đây để trả lời câu hỏi của người dùng.\n",
    "Nếu bạn không biết câu trả lời dựa trên thông tin được cung cấp, hãy nói rằng bạn không biết. Đừng cố bịa ra câu trả lời.\n",
    "Luôn trả lời bằng tiếng Việt.\n",
    "\n",
    "Thông tin tham khảo:\n",
    "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Câu hỏi: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "Câu trả lời hữu ích:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template_str, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "# --- 4. Tạo RetrievalQA chain ---\n",
    "print(\"\\nĐang tạo RetrievalQA chain...\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # \"stuff\" hoạt động tốt nếu tổng context + question < giới hạn của Llama\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}), # Lấy 3 chunks\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs # Sử dụng prompt tùy chỉnh\n",
    ")\n",
    "print(\"RetrievalQA chain đã được tạo.\")\n",
    "\n",
    "# --- 5. Vòng lặp hỏi đáp (GIỮ NGUYÊN) ---\n",
    "print(\"\\n--- CHATBOT DƯỢC SƯ TỰ SỰ (với Llama 3 - Ollama) ---\")\n",
    "print(\"Gõ 'thoát' để kết thúc phiên trò chuyện.\")\n",
    "print(\"Gõ 'nguồn' sau câu trả lời để xem các đoạn văn bản tham khảo.\")\n",
    "\n",
    "last_response_sources = None\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nBạn hỏi: \").strip()\n",
    "\n",
    "    if user_query.lower() == 'thoát':\n",
    "        break\n",
    "    if user_query.lower() == 'nguồn':\n",
    "        if last_response_sources:\n",
    "            print(\"\\n--- Nguồn tài liệu tham khảo cho câu trả lời trước: ---\")\n",
    "            for i, source_doc in enumerate(last_response_sources):\n",
    "                print(f\"\\nNguồn {i+1} (Từ chunk có metadata: {source_doc.metadata}):\")\n",
    "                content_preview = source_doc.page_content.replace(\"\\n\", \" \")[:300]\n",
    "                print(f\"   {content_preview}...\")\n",
    "            last_response_sources = None\n",
    "        else:\n",
    "            print(\"Chưa có câu trả lời nào trước đó để hiển thị nguồn.\")\n",
    "        continue\n",
    "\n",
    "    if not user_query:\n",
    "        continue\n",
    "\n",
    "    print(\"Chatbot đang suy nghĩ (Llama 3 đang xử lý)...\")\n",
    "    try:\n",
    "        result = qa_chain.invoke({\"query\": user_query})\n",
    "        print(\"\\nChatbot trả lời:\")\n",
    "        # ChatOllama thường trả về kết quả trong result['result'] hoặc trực tiếp là một AIMessage\n",
    "        # Nếu result là AIMessage thì result.content là câu trả lời\n",
    "        if isinstance(result.get(\"result\"), str):\n",
    "            print(result[\"result\"])\n",
    "        elif hasattr(result, 'content') and isinstance(result.content, str): # Cho trường hợp trả về AIMessage\n",
    "             print(result.content)\n",
    "        else:\n",
    "            # In ra để debug nếu cấu trúc khác\n",
    "            print(f\"Kết quả thô từ chain: {result}\")\n",
    "            # Nếu \"result\" là một dictionary chứa message object, bạn có thể cần truy cập sâu hơn\n",
    "            if isinstance(result.get(\"result\"), dict) and \"content\" in result.get(\"result\"):\n",
    "                 print(result.get(\"result\").get(\"content\"))\n",
    "            else:\n",
    "                 print(f\"Không thể trích xuất câu trả lời rõ ràng từ kết quả.\")\n",
    "\n",
    "\n",
    "        last_response_sources = result.get(\"source_documents\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi khi gọi Llama 3: {e}\")\n",
    "\n",
    "print(\"\\nCảm ơn bạn đã sử dụng chatbot! Tạm biệt.\")"
   ],
   "id": "492a6177c918aed9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang khởi tạo mô hình embedding: 'bkai-foundation-models/vietnamese-bi-encoder'...\n",
      "Mô hình embedding đã được khởi tạo.\n",
      "\n",
      "Đang tải FAISS index từ thư mục: 'DSTS_faiss_index_ch1-9'...\n",
      "FAISS index đã được tải thành công.\n",
      "\n",
      "Đang khởi tạo LLM (Ollama - Llama 3)...\n",
      "LLM Ollama (model: llama3:8b-instruct-q4_0) đã được khởi tạo.\n",
      "\n",
      "Đang tạo RetrievalQA chain...\n",
      "RetrievalQA chain đã được tạo.\n",
      "\n",
      "--- CHATBOT DƯỢC SƯ TỰ SỰ (với Llama 3 - Ollama) ---\n",
      "Gõ 'thoát' để kết thúc phiên trò chuyện.\n",
      "Gõ 'nguồn' sau câu trả lời để xem các đoạn văn bản tham khảo.\n",
      "Chatbot đang suy nghĩ (Llama 3 đang xử lý)...\n",
      "\n",
      "Chatbot trả lời:\n",
      "Theo thông tin được cung cấp, Miêu Miêu là một nô tì cấp bậc thấp nhất, chưa được nhận chức danh \"nữ quan\". Cô gái này có nước da rắn rỏi lấm tấm tàn nhang và tay chân khẳng khiu như cành củi khô. Miêu Miêu đang làm việc trong một phi tần hạ cấp và phải thu thập những nguyên liệu có thể dùng được từ ngăn kéo ngoài cùng trở đi.\n",
      "Chatbot đang suy nghĩ (Llama 3 đang xử lý)...\n",
      "\n",
      "Chatbot trả lời:\n",
      "Theo thông tin được cung cấp, Miêu Miêu không có hứng thú nào với những chuyện về tình yêu nam nữ trong hậu cung. Cô khoanh tay trầm ngâm bởi câu tự vấn của bản thân mình rằng \"Chẳng lẽ một ngày nào đó mình cũng như vậy?\" Điều này cho thấy Miêu Miêu không muốn trở thành người phụ nữ trong hậu cung, nơi tình yêu nam nữ cháy bỏng là không được phép.\n",
      "\n",
      "Ngoài ra, Miêu Miêu còn có những suy nghĩ khác về hậu cung, cô cảm thấy băn khoăn về việc các hoạn quan và cung nữ sau khi rời bỏ chức vụ lại trở thành vợ của nhau. Cô không thể hiểu sao một nơi như vậy lại có thể xảy ra những chuyện như thế.\n",
      "\n",
      "Tóm lại, Miêu Miêu ghét hậu cung vì cô không muốn trở thành người phụ nữ trong hậu cung và không chấp nhận cách sống của các hoạn quan và cung nữ trong đó.\n",
      "Chatbot đang suy nghĩ (Llama 3 đang xử lý)...\n",
      "\n",
      "Chatbot trả lời:\n",
      "Tôi không biết! Câu hỏi về Naruto không liên quan đến thông tin được cung cấp. Naruto là một nhân vật trong truyện tranh và anime Nhật Bản, nhưng không có mối liên hệ gì với văn bản được cung cấp.\n",
      "\n",
      "Cảm ơn bạn đã sử dụng chatbot! Tạm biệt.\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
